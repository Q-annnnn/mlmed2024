# -*- coding: utf-8 -*-
"""Lab2_HC_Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wI9WYSITOHu8rDUvsU0Ly5JSEoqeD7ot
"""

import pandas as pd
import numpy as np
import tensorflow as tf
import matplotlib.pylab as plt

import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.applications import ResNet50, VGG16
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, concatenate
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Dropout, Dense
from tensorflow.keras.preprocessing.image import img_to_array, load_img
from tqdm import tqdm
from keras import regularizers

from sklearn.model_selection import KFold
from sklearn.metrics import mean_absolute_error

from google.colab import drive
drive.mount('/content/drive')

"""Working with the meta data"""

df = pd.read_csv('/content/drive/MyDrive/training_set_pixel_size_and_HC.csv')
df.head()

df["img_path"] = '/content/drive/MyDrive/training_set/' + df['filename']
df.head()

def preprocess_images(image_paths, target_size=(512, 512)):
    images = []
    for path in tqdm(image_paths, desc='Processing images'):
        img = load_img(path, target_size=target_size)
        img_array = img_to_array(img)
        images.append(img_array)
    return np.array(images)

image_paths = [filename for filename in df['img_path']]
preprocessed_images = preprocess_images(image_paths)

preprocessed_images.shape

min_pixel_value = np.min(preprocessed_images)
max_pixel_value = np.max(preprocessed_images)

print(f"Minimum Pixel Value: {min_pixel_value}")
print(f"Maximum Pixel Value: {max_pixel_value}")

"""The number of pixels varies from 0 to 254

"""

# Normalization

preprocessed_images = preprocessed_images / 255.0

X_train, X_val, y_train, y_val = train_test_split(preprocessed_images, np.array(df['head circumference (mm)']), test_size=0.2, random_state=42)

def vgg16(input_shape, top='flatten'):
    if top not in ('flatten', 'avg', 'max'):
        raise ValueError('unexpected top layer type: %s' % top)
    print('The model is VGG16.')
    #base = VGG16(input_shape=input_shape, include_top=False)#train from scratch
    base = VGG16(weights='imagenet', input_shape=input_shape, include_top=False)
    x = base.output
    x = GlobalAveragePooling2D()(x) # Flatten | GlobalAveragePooling2D | GlobalMaxPooling2D
    x = Dropout(0.7)(x)
    x = Dense(512,kernel_regularizer=regularizers.l2(0.01),
                activity_regularizer=regularizers.l1(0.01))(x)
    x = Dense(256, kernel_regularizer=regularizers.l2(0.01),
              activity_regularizer=regularizers.l1(0.01))(x)
    x = Dense(128, kernel_regularizer=regularizers.l2(0.01),
              activity_regularizer=regularizers.l1(0.01))(x)
    x = Dense(64, kernel_regularizer=regularizers.l2(0.01),
              activity_regularizer=regularizers.l1(0.01))(x)
    x = Dense(32, kernel_regularizer=regularizers.l2(0.01),
              activity_regularizer=regularizers.l1(0.01))(x)
    pred = Dense(1, activation='linear')(x)
    model = Model(inputs=base.inputs, outputs=pred)

    '''
    Classif
    x = base.output
    x = GlobalAveragePooling2D()(x)
    x = Dropout(0.7)(x)
    predictions = Dense(num_classes, activation= 'softmax')(x)
    model = Model(inputs = base.input, outputs = predictions)
    '''
    return model

model = vgg16((512,512,3), top = 'flatten')

model.summary()

model.compile(optimizer=Adam(learning_rate = 0.0001), loss='mean_squared_error', metrics=['mae'])

checkpoint = ModelCheckpoint('/content/drive/MyDrive/best_model_weights.h5', save_best_only=True, save_weights_only=True, monitor='val_loss', mode='min', verbose=1)
early_stopping = EarlyStopping(patience=10, monitor='val_loss', mode='min', verbose=1)

history = model.fit(
    X_train, y_train,
    epochs=100,
    validation_data=(X_val, y_val),
    callbacks=[checkpoint, early_stopping]
)

# Save training history plot
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.savefig('/content/drive/MyDrive/training_history_loss.png')
plt.show()

# Save MAE plot
plt.plot(history.history['mae'], label='Training MAE')
plt.plot(history.history['val_mae'], label='Validation MAE')
plt.xlabel('Epochs')
plt.ylabel('MAE')
plt.legend()
plt.savefig('/content/drive/MyDrive/training_history_mae.png')
plt.show()

y_train

