# -*- coding: utf-8 -*-
"""mlmed.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MdQd0wzxV5tS-5Ay2CWux8yUliZMzl8Y
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.utils import resample
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, accuracy_score
from imblearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import GridSearchCV
from tensorflow.keras import layers, models
from tensorflow.keras import Sequential,utils
from tensorflow.keras.layers import Flatten, Dense, Conv1D, MaxPool1D, Dropout, BatchNormalization, MaxPooling1D
import tensorflow as tf
from sklearn.model_selection import RepeatedKFold
from keras.models import Sequential
from keras.layers import Dense
from sklearn.metrics import accuracy_score
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
from keras.callbacks import CSVLogger, ModelCheckpoint
from keras.utils import to_categorical
import keras

train_data = pd.read_csv('/content/drive/MyDrive/archive/mitbih_train.csv', header = None)
test_data = pd.read_csv('/content/drive/MyDrive/archive/mitbih_test.csv', header = None)
abnormal = pd.read_csv('/content/drive/MyDrive/archive/ptbdb_abnormal.csv', header = None)
normal = pd.read_csv('/content/drive/MyDrive/archive/ptbdb_normal.csv', header = None)

print('train shape:', train_data.shape,
      'test shape: ', test_data.shape,
      'abnormal shape: ', abnormal.shape,
      'normal shape: ', normal.shape)

train_label = train_data[187].astype(int)
test_label = test_data[187].astype(int)
train_info = train_data.iloc[:,:187]
test_info = test_data.iloc[:,:187]

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Count elements of each class
unique_classes, class_counts = np.unique(train_label, return_counts=True)

colors = sns.color_palette("Set2", len(unique_classes))
sns.set(style="whitegrid", font_scale=1.2)

# Plot
fig, ax = plt.subplots(figsize=(10, 6))
bars = ax.bar(unique_classes, class_counts, align='center', alpha=0.7, color=colors, edgecolor='black')

# Put the counting on top of the bars
for bar, count in zip(bars, class_counts):
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width() / 2, height + 0.05 * max(class_counts),
            f'{count}\n({100 * count / len(train_label):.1f}%)',
            ha='center', va='bottom', fontsize=14, color='black')

plt.xlabel('Class Labels', fontsize=14)
plt.ylabel('Count', fontsize=14)
plt.title('Distribution of Training Class Labels', fontsize=16, pad = 35)
ax.set_xticks(unique_classes)
ax.tick_params(axis='x', which='both', bottom=False)
plt.grid(axis='y', linestyle='--', alpha=0.7)


plt.show()

# Count elements
unique_classes, class_counts = np.unique(train_label, return_counts=True)

# Find class with least elements
min_class = unique_classes[np.argmin(class_counts)]

# Apply SMOTE
smote = SMOTE(sampling_strategy='auto', random_state=42)

# Resample the data
train_data_resampled, train_label_resampled = smote.fit_resample(train_info, train_label)

# Count elements
unique_classes, class_counts = np.unique(train_label_resampled, return_counts=True)

colors = sns.color_palette("Set2", len(unique_classes))
sns.set(style="whitegrid", font_scale=1.2)


fig, ax = plt.subplots(figsize=(10, 6))
bars = ax.bar(unique_classes, class_counts, align='center', alpha=0.7, color=colors, edgecolor='black')

for bar, count in zip(bars, class_counts):
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width() / 2, height + 0.05 * max(class_counts),
            f'{count}\n({100 * count / len(train_label_resampled):.1f}%)',
            ha='center', va='bottom', fontsize=14, color='black')

plt.xlabel('Class Labels', fontsize=14)
plt.ylabel('Count', fontsize=14)
plt.title('Distribution of Training Class Labels after applying SMOTE', fontsize=16, pad=35)
ax.set_xticks(unique_classes)
ax.tick_params(axis='x', which='both', bottom=False)
plt.grid(axis='y', linestyle='--', alpha=0.7)

plt.show()

model = DecisionTreeClassifier(random_state=42)

model.fit(train_data_resampled, train_label_resampled)

y_pred = model.predict(test_data.iloc[:,:187])

# Count the occurrences of each class
unique_classes, class_counts = np.unique(y_pred, return_counts=True)

# Plotting the histogram with count values on top of each bar
plt.bar(unique_classes, class_counts, align='center', alpha=0.7)
plt.xlabel('Class Labels')
plt.ylabel('Count')
plt.title('Histogram of Predicted Class Labels')

# Adding count values on top of each bar
for i, count in zip(unique_classes, class_counts):
    plt.text(i, count + 0.1, str(count), ha='center', va='bottom')

plt.show()

# Count the occurrences of each class
unique_classes, class_counts = np.unique(test_label, return_counts=True)

# Plotting the histogram with count values on top of each bar
plt.bar(unique_classes, class_counts, align='center', alpha=0.7)
plt.xlabel('Class Labels')
plt.ylabel('Count')
plt.title('Histogram of Actual Class Labels')

# Adding count values on top of each bar
for i, count in zip(unique_classes, class_counts):
    plt.text(i, count + 0.1, str(count), ha='center', va='bottom')

plt.show()

acc = accuracy_score(y_pred, test_label)

print(acc)

acc_multi = classification_report(y_pred, test_label)

print(acc_multi)

X_train, X_val, y_train, y_val = train_test_split(train_data_resampled, train_label_resampled, test_size=0.01, random_state=42)

def model():
    model = Sequential()
    model.add(Conv1D(filters=64, kernel_size=6, activation='relu',
                    padding='same', input_shape=(187, 1)))
    model.add(BatchNormalization())

    # adding a pooling layer
    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))

    model.add(Conv1D(filters=64, kernel_size=6, activation='relu',
                    padding='same', input_shape=(187, 1)))
    model.add(BatchNormalization())
    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))

    model.add(Conv1D(filters=64, kernel_size=6, activation='relu',
                    padding='same', input_shape=(187, 1)))
    model.add(BatchNormalization())
    model.add(MaxPooling1D(pool_size=(3), strides=2, padding='same'))

    model.add(Flatten())
    model.add(Dense(64, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(5, activation='softmax'))

    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

y_train1 = to_categorical(y_train)
y_val1 = to_categorical(y_val)

x_train = np.array(X_train).reshape(len(X_train),X_train.shape[1],1)
x_val = np.array(X_val).reshape(len(X_val),X_val.shape[1],1)

model = model()
model.summary()

logger = CSVLogger('logs.csv', append=True)
his = model.fit(x_train, y_train1, epochs=50, batch_size=32,
          validation_data=(x_val, y_val1), callbacks=[logger])

y_pred1 = np.argmax(model.predict(test_data.iloc[:,:187]), axis =1)

y_pred1

acc_multi1 = classification_report(y_pred1, test_label)

print(acc_multi1)

keras.utils.plot_model(
    model,
    to_file="model.png",
    show_shapes=True,
    show_dtype=False,
    show_layer_names=True,
    rankdir="TB",
    expand_nested=True,
    dpi=200,
    show_layer_activations=True,
    show_trainable=True
)

